{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6398a957",
   "metadata": {},
   "source": [
    "# Stock Price Prediction: ARIMA vs LSTM\n",
    "\n",
    "This notebook walks through the process of predicting stock prices using two different approaches: a traditional statistical model (ARIMA) and a deep learning model (LSTM). I'll be using simulated data that mimics historical stock prices to compare how these models handle trends and fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070a9c6",
   "metadata": {},
   "source": [
    "## 1. Setting Up and Getting Data\n",
    "First, I'll import the necessary libraries and generate some sample data. In a real-world scenario, I'd pull this from Yahoo Finance, but for this exercise, I'm creating a synthetic dataset with a clear trend and some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Generating synthetic stock data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2019-01-01', end='2023-12-31', freq='B')\n",
    "n = len(dates)\n",
    "price = 150 + np.cumsum(np.random.normal(0.1, 1.5, n)) + 10 * np.sin(np.linspace(0, 10, n))\n",
    "df = pd.DataFrame({'Date': dates, 'Adj Close': price})\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df['Adj Close'])\n",
    "plt.title('Historical Stock Price (Simulated)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ba884",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "Before feeding data into the models, I need to split it into training and testing sets. For the LSTM, I also need to normalize the values between 0 and 1 because neural networks are sensitive to the scale of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794093d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting 80/20\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_data, test_data = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "# Scaling for LSTM\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_train = scaler.fit_transform(train_data)\n",
    "scaled_test = scaler.transform(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2a226",
   "metadata": {},
   "source": [
    "## 3. ARIMA Model\n",
    "ARIMA is a classic for time series. I'll use a simple (5,1,0) configuration here. I'm using a rolling forecast approach where the model gets the actual previous value to predict the next one, which is a common way to evaluate these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at ACF/PACF to justify parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,4))\n",
    "plot_acf(df['Adj Close'].diff().dropna(), ax=ax1)\n",
    "plot_pacf(df['Adj Close'].diff().dropna(), ax=ax2)\n",
    "plt.show()\n",
    "\n",
    "# Running the ARIMA forecast\n",
    "history = [x for x in train_data['Adj Close']]\n",
    "predictions_arima = []\n",
    "for t in range(len(test_data)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.forecast()[0]\n",
    "    predictions_arima.append(yhat)\n",
    "    history.append(test_data.iloc[t]['Adj Close'])\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_data.index, test_data['Adj Close'], label='Actual')\n",
    "plt.plot(test_data.index, predictions_arima, color='red', label='ARIMA Prediction')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Forecast vs Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48435ca8",
   "metadata": {},
   "source": [
    "## 4. LSTM Model\n",
    "Now for the deep learning part. I'm setting up a sliding window of 60 days to predict the 61st. The model has a couple of LSTM layers with Dropout to prevent it from just memorizing the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=60):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 60\n",
    "full_scaled = np.concatenate((scaled_train, scaled_test), axis=0)\n",
    "X_train, y_train = create_dataset(scaled_train, look_back)\n",
    "X_test, y_test = create_dataset(full_scaled[train_size - look_back:], look_back)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Building the model\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(look_back, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# Training (keeping epochs low for the demo)\n",
    "history_lstm = model_lstm.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Predictions\n",
    "predictions_lstm = model_lstm.predict(X_test)\n",
    "predictions_lstm = scaler.inverse_transform(predictions_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c8751",
   "metadata": {},
   "source": [
    "## 5. Comparison and Results\n",
    "Finally, let's see how they stack up. I'll calculate the standard error metrics and plot everything together. Usually, ARIMA is better at short-term trends while LSTM can sometimes pick up on more complex, non-linear patterns if given enough data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b4af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_data['Adj Close'].values\n",
    "pred_arima = np.array(predictions_arima)\n",
    "pred_lstm = predictions_lstm.flatten()\n",
    "\n",
    "def get_metrics(a, p):\n",
    "    return mean_absolute_error(a, p), np.sqrt(mean_squared_error(a, p))\n",
    "\n",
    "mae_a, rmse_a = get_metrics(actual, pred_arima)\n",
    "mae_l, rmse_l = get_metrics(actual, pred_lstm)\n",
    "\n",
    "print(f\"ARIMA -> MAE: {mae_a:.2f}, RMSE: {rmse_a:.2f}\")\n",
    "print(f\"LSTM  -> MAE: {mae_l:.2f}, RMSE: {rmse_l:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(test_data.index, actual, label='Actual Price', alpha=0.7)\n",
    "plt.plot(test_data.index, pred_arima, label='ARIMA', linestyle='--')\n",
    "plt.plot(test_data.index, pred_lstm, label='LSTM', linestyle=':')\n",
    "plt.title('Model Comparison: ARIMA vs LSTM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
